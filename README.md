# Interpretable-named-entity-recognition-with-keras-LSTM-and-LIME

Deep neural networks are quite successful in many use-cases, but these models can be hard to debug and to understand whatâ€™s going on. My aim is to understand how much certain words influence the prediction of our named entity tagger. I want a human-understandable qualitative explanation which enables an interpretation of the underlying algorithm.

link for the dataset : https://www.kaggle.com/namanj27/ner-dataset
